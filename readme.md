# Recommended Hardware Configuration

### Minimum System Requirements
- **Processor**: Modern multi-core CPU (4+ cores recommended)
- **RAM**: 16GB minimum
- **Storage**: 10GB for application code and dependencies
- **GPU**: CUDA-compatible GPU with 8GB+ VRAM recommended

### Recommended Configuration
- **Processor**: 8+ core CPU (e.g., Intel i7/i9 or AMD Ryzen 7/9)
- **RAM**: 32GB or more
- **Storage**: SSD with at least 50GB of free space
- **GPU**: NVIDIA GPU with 16GB+ VRAM (e.g., RTX 3080 or better)

> **Note**: While lower configurations are viable, performance may be compromised, leading to longer execution times and potential system slowdowns.

### Cloud Deployment Alternative
If deploying to cloud infrastructure:
- Standard virtual machine with 8+ vCPUs
- 32GB RAM
- GPU acceleration if available

# App Installation

To install the app, we have to do the following:

1. Install Mini-Conda on your computer (if already not installed). The following link can be used for installation: [LINK](https://docs.anaconda.com/miniconda/install/#quick-command-line-install).

2. Clone the repo using git:
   ```
   https://github.com/Nafiz43/EvidenceBot
   ```

3. Create and activate a new virtual environment:
   ```bash
   conda create -n EvidenceBot python=3.10.0
   conda activate EvidenceBot
   ```

4. Install all the requirements:
   ```bash
   pip install -r requirements.txt
   ```

5. Install Ollama from the following [LINK](https://ollama.com/download).

6. Install Models using Ollama:
   ```bash
   ollama pull MODEL_NAME
   ```
   Replace MODEL_NAME with your desired model name. List of all the models is available at this [link](https://ollama.com/download).

7. Open the source directory, where the source code exists. Then, keep the documents that you want to analyze in the DATA_DOCUMENTS folder.

8. Open CLI (Command Line Interface) in the source directory and hit the following command:
   ```bash
   ollama pull MODEL_NAME
   ```

9. To run the app, navigate to the project directory and execute the following command in the command line:
   ```bash
   sh command.sh
   ```

## Running the Application

### Generating Individual Responses
The application is, by default, accessible at the following [link](localhost:8501) on the local machine. In this mode, users can interact with the system by providing a specific prompt to receive responses generated by a selected Large Language Model (LLM). Additionally, the application provides flexibility by allowing users to adjust several parameters governing the behavior of the model. A complete list of these parameters, along with their default values, can be found in Section 3 (*Application Parameters*). Unless modified by the user, the application will run with the default parameter values, meaning that simply providing a prompt will invoke selected LLM using these predefined settings.

The model selection field within the application is dynamic and automatically detects and displays the names of the LLMs installed on the local machine. For example, if five models are installed, these five models will be shown for selection. Moreover, for each session, the application generates a vector database from the contents of the `DATA_DOCUMENTS` directory. It is crucial for users to ensure that relevant contextual documents are stored within this directory. This vector database is created the first time the user submits a query, and the system will retrieve relevant text chunks from this database to ameliorate the model's response. The system selects the most contextually appropriate text chunks to pass to the model in order to enhance the quality and relevance of the response.

If any changes are made to the parameters during the session, it is necessary to restart the application to ensure that these updates are reflected in the vector database.

![Generating Individual Responses Functionality](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/gen_ind_pre.jpeg)
*Figure 1: Generate Individual Responses Functionality*

Upon submission of a prompt by the user, the application processes it and generates a response based on the selected model. A sample output is displayed in Figure 2. In this example, the generated response is presented alongside the source content. The source content refers to the text chunks extracted from the vector database that are passed to the LLM as context to facilitate a more informed and relevant response. The user can view both the model's output and the associated source content for each query. Furthermore, the model's response is logged in the `logs` directory in the `history.csv` file, along with the corresponding timestamp. This logging functionality provides users with the ability to track and compare responses over time, as the logs store both the original prompt and the generated response for each session.

![Generating Individual Responses [Output]](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/gen_ind_post.jpeg)
*Figure 2: Generate Individual Responses Output*

### Generating Batch Responses
When the user selects the *Batch Question Mode* radio button, the application switches to batch processing mode, as depicted in Figure 3. In this mode, users can upload a .csv file containing a list of questions, which will be processed sequentially by the LLM model. Each question is handled individually, with the system generating a response for each one in turn.

![Generating Batch Responses Functionality](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/gen_batch_pre.jpeg)
*Figure 3: Generate Batch Responses Functionality*

The system expects the .csv file to contain a single column of questions. After the user uploads the file, the application provides an overview of the content, displaying the questions to ensure they are properly formatted for processing. Once confirmed, the questions are processed one by one.

For the batch response process, the application creates a vector database from the contents of the `DATA_DOCUMENTS` folder at the start of the batch job. During processing, the system also tracks the progress and displays the number of queries that have already been processed, as shown in Figure 4.

![Generating Batch Responses [Output]](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/gen_batch_post.jpeg)
*Figure 4: Generate Batch Responses Functionality*

The results of the batch processing are stored in the `logs` directory in a .csv file titled `history.csv`. This output file includes the questions from the .csv file along with the corresponding responses generated by the model. A sample of the output file is shown in Figure 5.

![Generating Batch Responses [Output-File-Format]](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/gen_batch_post2.jpeg)
*Figure 5: Generate Batch Responses Functionality*

### Evaluating Individual Responses
Users can access the *Evaluate Individual Response* mode by selecting the `Evaluate` option from the navigation bar. By default, the application will run on the following [link](localhost:8502) on the local machine. This mode enables users to quantitatively assess the differences between a model-generated response and a reference text. Specifically, it requires two inputs from the user: the reference text and the candidate (model-generated) text, as illustrated in Figure 6.

![Evaluating Individual Responses](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/evaluate_ind_pre.jpeg)
*Figure 6: Evaluate Individual Responses Functionality*

The system evaluates the difference between the two provided texts and displays the results through four evaluation metrics: BLEU-4, ROUGE-L, BERTScore, and Cosine Similarity. These metrics provide different perspectives on how closely the model's output aligns with the reference text, offering insights into various aspects of text quality, such as precision, recall, and semantic similarity. A sample output of this evaluation is shown in Figure 7, where users can view the metric values along with a visual representation of the evaluation results.

![Evaluating Individual Responses [Output]](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/evaluate_ind_post.jpeg)
*Figure 7: Evaluate Individual Responses [Output]*

### Evaluating Batch Responses
The *Evaluate Batch Response* mode allows users to assess a batch of responses generated by the model. To activate this mode, users must select the `Batch Response Evaluation Mode`, as shown in Figure 8. This mode expects two .csv files as input: one containing the reference texts (`reference.csv`) and the other containing the candidate (model-generated) responses (`candidate.csv`).

![Evaluating Batch Responses [Output]](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/evaluate_batch_pre.jpeg)
*Figure 8: Evaluate Batch Responses*


Upon selecting the files, the system displays an overview of the uploaded files to ensure the user has selected the correct input. Once the files are confirmed, the system processes them and computes the evaluation metrics for each pair of reference and candidate texts. The results are visualized as a colorful bar graph, which provides an intuitive comparison of the metric values across the batch (See Figure 9). This graphical representation helps users quickly assess the overall quality of the responses across different metrics.

![Evaluating Batch Responses [Output]](https://github.com/Nafiz43/EvidenceBot/blob/main/figs/evaluate_batch_post.jpeg)
*Figure 9: Evaluate Batch Responses [Output]*

## App Parameters
The user can set three RAG-based parameters and eight model parameters in the app. The RAG-based parameters are as follows:

1. *Embedding_model_name:*  
   Refers to the name or identifier of the embedding model employed to generate vector representations of the input data. This model is critical in tasks involving similarity searches, semantic understanding, or clustering, as it transforms raw data (e.g., text) into numerical embeddings that capture meaningful relationships and contextual semantics. Examples include `openai/text-embedding-ada-002` or custom-trained models.

2. *CHUNK_SIZE:*  
   Defines the size of each segment or chunk of text that is processed or embedded individually. This parameter determines how input data is divided into smaller, manageable pieces, typically measured in terms of characters, words, or tokens. Choosing an appropriate chunk size is essential to balance between capturing enough context in each chunk and avoiding excessive data redundancy or model limitations.

3. *K (entries to retrieve):*  
   Specifies the number of top results or nearest neighbors to return during similarity search or retrieval operations. This parameter determines how many entries from the database are considered most relevant to the input query, based on their similarity scores. Higher values of *K* provide broader results, while lower values focus on the most relevant matches.

The Model parameters are as follows:

1. *temp (Temperature):*  
   Controls the randomness of the model's output. A higher value (e.g., `1.0`) results in more diverse outputs, while a lower value (e.g., `0.2`) makes the output more deterministic and focused. Default: `1.0`.

2. *top_p (Nucleus Sampling):*  
   Regulates the probability mass of the tokens considered for sampling. The model selects from the smallest possible set of tokens whose cumulative probability is greater than or equal to `top_p`. A value of `0.9` typically balances diversity and relevance. Default: `0.9`.

3. *top_k (Top-K Sampling):*  
   Limits the model to sampling from the top `K` most likely tokens at each step. For example, setting `top_k = 50` considers only the top 50 tokens for generating the next word. Default: `40`.

4. *tfs_z (Tail Free Sampling):*  
   Adjusts sampling by filtering tokens based on their tail free distribution. This helps to maintain a balance between creative and coherent output. A value of `1.0` retains all tokens, while lower values filter out less probable tokens. Default: `1.0`.

5. *num_ctx (Context Length):*  
   Defines the maximum number of tokens the model considers as context for generating predictions. A higher value allows the model to consider more history but increases computational cost. Default: `2048` tokens (may vary by model).

6. *repeat_penalty:*  
   Discourages the model from repeating the same tokens by applying a penalty to their probability during sampling. A value greater than `1.0` (e.g., `1.1`) reduces repetition, while `1.0` disables the penalty. Default: `1.1`.

7. *mirostat:*  
   A dynamic sampling technique aimed at maintaining a target perplexity during text generation. This helps to produce outputs with consistent quality and coherence. Default: `0` (disabled).

8. *mirostat_eta:*  
   The learning rate for updating the perplexity in Mirostat. It determines how quickly the algorithm adjusts to achieve the target perplexity. Default: `0.1`.

9. *mirostat_tau:*  
   The target perplexity value for Mirostat. This sets the desired balance between diversity and coherence in the generated text. Default: `5.0`.

## Video Demonstration
Here is the link to the video demonstration: [Link](https://www.youtube.com/watch?v=oZZRDebxTBg)
